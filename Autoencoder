# -*- coding: utf-8 -*-
"""
Created on Sat Feb 14 19:39:29 2026

@author: Oilsmell
"""

# -*- coding: utf-8 -*-
"""
[Step 1] Scale-Invariant Autoencoder Training
Strategy: Individual Normalization (Separate Scalers for A and B)
Goal: Train AE on normalized (0-1) data from both structures to learn common waveform features.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import os
import pickle
from sklearn.preprocessing import MinMaxScaler

# =========================================================
# 1. Configuration
# =========================================================
class Config:
    # 경로 설정
    DIR_A = r"E:\Benchmark Code\benchmarktu1402-master\f_accerlerations\ds1"
    FILE_A = "fh_accelerations.dat"
    
    DIR_B = r"E:\2ndstructuredata\raw data"
    FILE_B = "healthyclean.txt"
    
    SAVE_DIR = r"E:\2ndstructuredata\Code_2"
    
    # 모델 및 스케일러 저장 이름
    MODEL_NAME = "autoencoder_model.pth"
    SCALER_A_NAME = "scaler_A.pkl" # 구조물 A 전용 스케일러
    SCALER_B_NAME = "scaler_B.pkl" # 구조물 B 전용 스케일러
    
    # 하이퍼파라미터
    WINDOW_SIZE = 128
    LATENT_DIM = 8
    BATCH_SIZE = 512
    EPOCHS = 200
    LR = 0.001
    SELECTED_NODES = [3, 21, 39, 57, 63, 81, 99, 117] # 8 Sensors
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

cfg = Config()

if not os.path.exists(cfg.SAVE_DIR):
    os.makedirs(cfg.SAVE_DIR)

# =========================================================
# 2. Data Loading & Processing Utilities
# =========================================================
def load_structure_A(filepath):
    """Load Structure A (Dat format)"""
    print(f"   -> Loading Structure A from {filepath}...")
    try:
        data = np.loadtxt(filepath)
        # (Time, Nodes) -> (Nodes, Time) if needed
        if data.shape[0] > data.shape[1]:
            data = data[:, cfg.SELECTED_NODES].T
        else:
            data = data[cfg.SELECTED_NODES, :]
        return data.astype(np.float32) # Shape: (8, Time)
    except Exception as e:
        print(f"Error loading A: {e}")
        return None

def load_structure_B(filepath):
    """Load Structure B (Txt format)"""
    print(f"   -> Loading Structure B from {filepath}...")
    try:
        raw = []
        with open(filepath, 'r') as f:
            for line in f:
                p = line.split()
                if len(p) >= 2: raw.append(float(p[1]))
        data = np.array(raw, dtype=np.float32).reshape(8, -1)
        return data # Shape: (8, Time)
    except Exception as e:
        print(f"Error loading B: {e}")
        return None

def process_and_normalize(data, scaler_name):
    """
    1. 데이터를 (N_samples, 8*Window) 형태로 변환
    2. MinMaxScaler로 0~1 정규화 (Fit & Transform)
    3. 스케일러 저장
    """
    n_sensors, n_points = data.shape
    n_samples = n_points // cfg.WINDOW_SIZE
    valid_len = n_samples * cfg.WINDOW_SIZE
    
    # (8, Time) -> (8, N, Window) -> (N, 8, Window) -> Flatten (N, 8*Window)
    data = data[:, :valid_len]
    reshaped = data.reshape(n_sensors, n_samples, cfg.WINDOW_SIZE).transpose(1, 0, 2).reshape(n_samples, -1)
    
    # [핵심] 0~1 정규화 (Normalization)
    print(f"   -> Normalizing {scaler_name} (Range 0-1)...")
    scaler = MinMaxScaler(feature_range=(0, 1))
    normalized_data = scaler.fit_transform(reshaped)
    
    # 스케일러 저장
    save_path = os.path.join(cfg.SAVE_DIR, scaler_name)
    with open(save_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   -> Scaler saved to {scaler_name}")
    
    return normalized_data

# =========================================================
# 3. Model Definition
# =========================================================
class Autoencoder(nn.Module):
    def __init__(self, input_dim=128*8, latent_dim=8):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512), nn.Tanh(),
            nn.Linear(512, 256), nn.Tanh(),
            nn.Linear(256, 64), nn.Tanh(),
            nn.Linear(64, latent_dim) # Output: z
        )
        # Decoder (대칭 구조)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64), nn.Tanh(),
            nn.Linear(64, 256), nn.Tanh(),
            nn.Linear(256, 512), nn.Tanh(),
            nn.Linear(512, input_dim), nn.Sigmoid() # [중요] 0~1 출력을 위해 Sigmoid 사용
        )
        
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

# =========================================================
# 4. Main Training Loop
# =========================================================
def main():
    print("[Step 1] Data Preparation (Individual Normalization)")
    
    # 1. Load Data
    raw_A = load_structure_A(os.path.join(cfg.DIR_A, cfg.FILE_A))
    raw_B = load_structure_B(os.path.join(cfg.DIR_B, cfg.FILE_B))
    
    if raw_A is None or raw_B is None:
        print("❌ Data loading failed.")
        return

    # 2. Individual Normalization (따로따로 정규화)
    norm_A = process_and_normalize(raw_A, cfg.SCALER_A_NAME)
    norm_B = process_and_normalize(raw_B, cfg.SCALER_B_NAME)
    
    # 3. Combine Data (A + B)
    # 이제 둘 다 0~1 범위이므로 합쳐도 문제 없음
    combined_data = np.vstack([norm_A, norm_B])
    print(f"\n[Data Info] Structure A: {norm_A.shape[0]} samples")
    print(f"[Data Info] Structure B: {norm_B.shape[0]} samples")
    print(f"[Data Info] Total Training Samples: {combined_data.shape[0]}")
    
    # 4. Create DataLoader
    dataset = TensorDataset(torch.FloatTensor(combined_data))
    loader = DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)
    
    # 5. Initialize Model
    print(f"\n[Step 2] Training Autoencoder on {cfg.device}...")
    model = Autoencoder(input_dim=cfg.WINDOW_SIZE*8, latent_dim=cfg.LATENT_DIM).to(cfg.device)
    optimizer = optim.Adam(model.parameters(), lr=cfg.LR)
    criterion = nn.MSELoss() # 0~1 사이 값의 복원 오차 최소화
    
    # 6. Training Loop
    model.train()
    for epoch in range(cfg.EPOCHS):
        total_loss = 0
        for batch in loader:
            x = batch[0].to(cfg.device)
            
            optimizer.zero_grad()
            recon, z = model(x)
            loss = criterion(recon, x)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        if (epoch+1) % 20 == 0:
            avg_loss = total_loss / len(loader)
            print(f"   Epoch {epoch+1}/{cfg.EPOCHS} | MSE Loss: {avg_loss:.6f}")
            
    # 7. Save Model
    save_path = os.path.join(cfg.SAVE_DIR, cfg.MODEL_NAME)
    torch.save(model.state_dict(), save_path)
    # 인코더만 따로 저장 (나중에 쓰기 편하게)
    torch.save(model.encoder.state_dict(), os.path.join(cfg.SAVE_DIR, "encoder_only.pth"))
    
    print(f"\n✅ Training Complete. Model saved to {save_path}")
    print(f"✅ Scalers saved: {cfg.SCALER_A_NAME}, {cfg.SCALER_B_NAME}")

if __name__ == "__main__":
    main()
